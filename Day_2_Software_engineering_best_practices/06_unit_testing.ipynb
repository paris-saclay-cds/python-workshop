{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=700px; src=\"../img/logoUPSayPlusCDS_990.png\">\n",
    "\n",
    "<p style=\"margin-top: 3em; margin-bottom: 2em;\"><b><big><big><big><big>Unit testing</big></big></big></big></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "1. [Testing principles](#1.-General-testing-principles)\n",
    "2. [Unit tests](#2.-Unit-tests)\n",
    "3. [Writing a first test](#3.-Writing-a-test-with-pytest)\n",
    "4. [Configuring `pytest`](#Pytest-configuration)\n",
    "5. [Test coverage](#Test-coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foreword\n",
    "\n",
    "There are several Python libraries dedicated to unit testing. \n",
    "The most common are:\n",
    "\n",
    "* [`unittest`](http://docs.python.org/library/unittest.html)\n",
    "* [`nose`](http://nosetest.org)\n",
    "* [`pytest`](http://pytest.org/)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>ATTENTION!</b>: <br><br>\n",
    "This course focuses on the use of `pytest`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General testing principles\n",
    "\n",
    "### Why should you write tests ?\n",
    "\n",
    "In general, tests are an **assessment of** both the **quality** and the **efficiency** of your code.\n",
    "\n",
    "Tests actually **define** the requirements of the code at various levels. From the basic method definition, to the full software validation.\n",
    "\n",
    "### What kind of tests should you write ?\n",
    "\n",
    "For each of these levels, a type of test exists :\n",
    "- unit tests\n",
    "- non-regression tests\n",
    "- pre-integration tests\n",
    "- integration tests\n",
    "- validation tests\n",
    "\n",
    "From this terminology, the unit tests are the basic elements, that should be run **before any commit** of the code. \n",
    "They are the one that will be the focus of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unit tests\n",
    "\n",
    "### Definition\n",
    "\n",
    "A unit test case should \n",
    "- test ** individual ** software components (\"units\") like classes and methods,\n",
    "- supply mocks or fake versions of dependencies (e.g. database, server, etc.) so that the test does ** not rely on any external object **,\n",
    "- enable failures to be pinpointed easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design (theory)\n",
    "\n",
    "Based on :\n",
    "\n",
    "    “The Art of Software Testing”, Glenford J. Myers, Corey Sandler, Tom Badgett, Wiley & Sons, 2011\n",
    "\n",
    "#### Normal conditions\n",
    "\n",
    "* **Rightness** validate the results against the requirements\n",
    "  \n",
    "  *Ex:* a method supposed to select the largest number of a list should be checked by comparing the result with the identified maximum from a known list.\n",
    "  \n",
    "\n",
    "* ** Inverse relationships ** \n",
    "\n",
    "  *Ex1:* a method calculating the square root of a number can be checked by squaring the result.\n",
    "  *Ex2:* a method inserting a value in a file can be checked by searching this value after insertion.\n",
    "  \n",
    "  This should be done with tools independent from the method to test (other library).\n",
    "\n",
    "\n",
    "* ** Cross-check: **\n",
    "\n",
    "  *Ex:* an analytical result can be compared to a numerical calculations for values or conditions where it is possible.\n",
    "\n",
    "\n",
    "* ** Code logic: **\n",
    "\n",
    "  *Ex:* \n",
    "  ```python\n",
    "  if ((x or y) and z):\n",
    "     decision_1\n",
    "  else: \n",
    "     decision_2\n",
    "  ``` \n",
    "  where `x`, `y `and `z` are called \"conditions\".\n",
    "  \n",
    "  Combinations of conditions could be unexpected and lead to a wrong decision. Some decisions are never reached because a different combinations of conditions always lead to the same logical value.\n",
    "\n",
    "#### Abnormal conditions and edges\n",
    "\n",
    "* ** Exceptions: **\n",
    "\n",
    "  If the methods throws exceptions under a certain conditions, this should be checked with a test. See in [section 3](#3.-Writing-a-test-with-pytest).\n",
    "  \n",
    "  ```python\n",
    "  with pytest.raises(<NameOfException>):\n",
    "      # call to the method that should throw the exception\n",
    "  ```\n",
    "  \n",
    "\n",
    "* ** Boundary conditions: **\n",
    "\n",
    "  - Does the value exist ?\n",
    "  - Does the value conform to an expected format ?\n",
    "  - Is the value given in a reasonable range (min, max) ?\n",
    "  - Are there enough values (cardinality) ?\n",
    "  - Does the code reference anything external ?\n",
    "  - What are the edges of the partitioned values ?\n",
    "  - Is everything happening in the right order ?\n",
    "\n",
    "\n",
    "* ** Error conditions: **\n",
    "\n",
    "  Force error conditions\n",
    "  - running out of memory\n",
    "  - running out of disk space\n",
    "  - issues with wall-clock time\n",
    "  - network availability and errors\n",
    "  - system load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Driven Development\n",
    "\n",
    "Test-driven development (TDD) is a software development process, part of the [Agile](https://en.wikipedia.org/wiki/Agile_software_development) principles, that relies on the transcription of the software requirements into tests, before the code that passes the tests is written.\n",
    "\n",
    "It is an ** iterative process ** that aims at starting with a basic test case andthen ** upgrading alternatively ** the test cases and the code until the requirements are met, depending on the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing a test with `pytest`\n",
    "\n",
    "A unit test written under `pytest` is a Python function or class whose name **starts with \"test\"** and that makes an hypothesis ones considers true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first test\n",
    "\n",
    "Let's right a basic file containing a function f, and the corresponding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file my_first_test.py\n",
    "\n",
    "def f(a):\n",
    "    return a\n",
    "\n",
    "def test_a():\n",
    "    assert f(1) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has been saved in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching pytest is as easy as move to the right directory and using the command line\n",
    "\n",
    "    py.test\n",
    "    \n",
    "It will start a recursive search from the current directory for Python files, look for methods containing \"test\" and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick summary, use the quick option `-q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py.test -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on which test has been run, use the verbose option `-v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py.test -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic test `test_a` has passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional tests\n",
    "\n",
    "Let's now write a bunch of tests, introduce an error on `test_b` and re-run pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file my_second_test.py\n",
    "\n",
    "def f(a):\n",
    "    return a\n",
    "\n",
    "def test_a():\n",
    "    assert f(1) == 1\n",
    "    \n",
    "def test_b():\n",
    "    assert f(2) == 1\n",
    "\n",
    "def test_c():\n",
    "    assert f(3) == 1 + 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py.test -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see pytest has *collected* and run 4 items, 1 from the first file, and 3 from the second. \n",
    "\n",
    "As expected, one test has failed.\n",
    "\n",
    "Therefore `pytest` shows the full traceback leading to the failure, and even gives the output value of the `f` method which can be useful for quick debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing errors and exceptions\n",
    "\n",
    "The philosophy of Python is to try something first and then decide what to do in case of an error. This is the reason behind Python Exceptions. They inform on the issue that was detected and help the user debug or catch it and find another way to deal with the issue.\n",
    "\n",
    "When testing a code, it is thus important to assess if these Exceptions are raised as they should be. However, since an exception raised but not caught in an environmment triggers an error, one cannot use the \"assert\" syntax but the context manager `pytest.raises` instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file my_third_test.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "def h(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Negative value detected\")\n",
    "    return n\n",
    "        \n",
    "def test_h():\n",
    "    assert h(1) == 1\n",
    "    \n",
    "def test_exception_h():\n",
    "    with pytest.raises(ValueError):\n",
    "        h(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py.test -v my_third_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest configuration\n",
    "\n",
    "This part explains how to organize your tests for testing a module. It uses the `euclid` toy module created for this course and available in the base directory under `python-euclid2016/euclid`. \n",
    "\n",
    "At this point, it is easier to open a separate terminal, go to the `euclid` directory\n",
    "\n",
    "    cd ~/Desktop/python-euclid2016/euclid  # for the VM users\n",
    "\n",
    "and continue from there.\n",
    "\n",
    "***Remainder***: shell commands in the notebook are preceded with \"!\", **not** in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Depending on where you are at this point do not run this\n",
    "%cd ../euclid/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directory contains a `Makefile` with three utility commands:\n",
    "\n",
    "    make clean     # to remove __pycache__ and .pyc files\n",
    "    make tests     # to run the tests\n",
    "    make coverage  # to run coverage tests (see next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!make clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In order to **visualize** the arborescence of the module and test directory, I recommend using the Linux utility `tree` which can be install in the VM with \n",
    "\n",
    "    sudo yum install -y tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except from the `Makefile`, there are two directories, the module `euclid` and the test directory `tests`.\n",
    "\n",
    "The `tests` directory contains \n",
    "\n",
    "* `__init__.py`  an **empty** file so that the tests are aware of the `euclid` model,\n",
    "* `test_mytrigo.py`  a file containing the tests for the functions in `mytrigo.py`\n",
    "* `conftest.py`  a pytest configuration file whose purpose is to **host the fixtures** for all tests\n",
    "\n",
    "***Note*** `conftest.py` do not need to be imported for pytest to use the fixtures. It is automatic.\n",
    "\n",
    "---\n",
    "\n",
    "### Content of `conftest.py`\n",
    "\n",
    "To visualize the content of the files, one can use\n",
    "\n",
    "    %load myfile.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test coverage\n",
    "\n",
    "A test coverage is a report on the number of lignes of a module that have been tested. Basically, a good coverage means much of your code has been run at least once during a test.\n",
    "\n",
    "To use coverage with pytest, one must first install\n",
    "    \n",
    "    sudo pip install pytest-cov\n",
    "\n",
    "Then the coverage test is run on the module, not on the tests itself. Here the module is `euclid`. before running the coverage test, one needs to be in the directory \n",
    "\n",
    "    ../python-euclid2016/euclid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!py.test --cov euclid/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that `hello.py` and `mytime.py` are not covered by tests.\n",
    "\n",
    "However, the coverage of `mytime.py` is not 0, as all of the `__init__.py` since the **imports** present in the files trigger an evaluation of some of the lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
