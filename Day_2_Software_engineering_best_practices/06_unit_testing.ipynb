{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=700px; src=\"../img/logoUPSayPlusCDS_990.png\">\n",
    "\n",
    "<p style=\"margin-top: 3em; margin-bottom: 2em;\"><b><big><big><big><big>Unit testing</big></big></big></big></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "1. [Testing principles](#1.-General-testing-principles)\n",
    "2. [Unit tests](#2.-Unit-tests)\n",
    "3. [Writing a test with pytest](#3.-Writing-a-test-with-pytest)\n",
    "4. [Writing tests for spectra_analysis](#4.-Writing-tests-for-the-spectra_analysis-module)\n",
    "5. [Test coverage](#5.-Test-coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foreword\n",
    "\n",
    "There are several Python libraries dedicated to unit testing. \n",
    "The most common are:\n",
    "\n",
    "* [`unittest`](http://docs.python.org/library/unittest.html)\n",
    "* [`nose`](http://nosetest.org)\n",
    "* [`pytest`](http://pytest.org)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "This course focuses on the use of `pytest`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General testing principles\n",
    "\n",
    "### Why should you write tests ?\n",
    "\n",
    "In general, tests are an **assessment of** both the **quality** and the **efficiency** of your code.\n",
    "\n",
    "Tests actually **define** the requirements of the code at various levels. From the basic method definition, to the full software validation.\n",
    "\n",
    "### What kind of tests should you write ?\n",
    "\n",
    "For each of these levels, a type of test exists :\n",
    "- unit tests\n",
    "- non-regression tests\n",
    "- pre-integration tests\n",
    "- integration tests\n",
    "- validation tests\n",
    "\n",
    "From this terminology, the unit tests are the basic elements, that should be run **before any commit** of the code. \n",
    "They are the one that will be the focus of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unit tests\n",
    "\n",
    "### Definition\n",
    "\n",
    "A unit test case should \n",
    "- test ** individual ** software components (\"units\") like classes and methods,\n",
    "- supply mocks or fake versions of dependencies (e.g. database, server, etc.) so that the test does ** not rely on any external object **,\n",
    "- enable failures to be pinpointed easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design (theory)\n",
    "\n",
    "Based on :\n",
    "\n",
    "    “The Art of Software Testing”, Glenford J. Myers, Corey Sandler, Tom Badgett, Wiley & Sons, 2011\n",
    "\n",
    "#### Normal conditions\n",
    "\n",
    "* **Rightness** validate the results against the requirements\n",
    "  \n",
    "  *Ex:* a method supposed to select the largest number of a list should be checked by comparing the result with the identified maximum from a known list.\n",
    "  \n",
    "\n",
    "* ** Inverse relationships ** \n",
    "\n",
    "  *Ex1:* a method calculating the square root of a number can be checked by squaring the result.\n",
    "  *Ex2:* a method inserting a value in a file can be checked by searching this value after insertion.\n",
    "  \n",
    "  This should be done with tools independent from the method to test (other library).\n",
    "\n",
    "\n",
    "* ** Cross-check: **\n",
    "\n",
    "  *Ex:* an analytical result can be compared to a numerical calculations for values or conditions where it is possible.\n",
    "\n",
    "\n",
    "* ** Code logic: **\n",
    "\n",
    "  *Ex:* \n",
    "  ```python\n",
    "  if ((x or y) and z):\n",
    "     decision_1\n",
    "  else: \n",
    "     decision_2\n",
    "  ``` \n",
    "  where `x`, `y `and `z` are called \"conditions\".\n",
    "  \n",
    "  Combinations of conditions could be unexpected and lead to a wrong decision. Some decisions are never reached because a different combinations of conditions always lead to the same logical value.\n",
    "\n",
    "#### Abnormal conditions and edges\n",
    "\n",
    "* ** Exceptions: **\n",
    "\n",
    "  If the methods throws exceptions under a certain conditions, this should be checked with a test. See [this section](#Testing-errors-and-exceptions).\n",
    "  \n",
    "  ```python\n",
    "  with pytest.raises(<NameOfException>):\n",
    "      # call to the method that should throw the exception\n",
    "  ```\n",
    "  \n",
    "\n",
    "* ** Boundary conditions: **\n",
    "\n",
    "  - Does the value exist ?\n",
    "  - Does the value conform to an expected format ?\n",
    "  - Is the value given in a reasonable range (min, max) ?\n",
    "  - Are there enough values (cardinality) ?\n",
    "  - Does the code reference anything external ?\n",
    "  - What are the edges of the partitioned values ?\n",
    "  - Is everything happening in the right order ?\n",
    "\n",
    "\n",
    "* ** Error conditions: **\n",
    "\n",
    "  Force error conditions\n",
    "  - running out of memory\n",
    "  - running out of disk space\n",
    "  - issues with wall-clock time\n",
    "  - network availability and errors\n",
    "  - system load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Driven Development\n",
    "\n",
    "Test-driven development (TDD) is a software development process, part of the [Agile](https://en.wikipedia.org/wiki/Agile_software_development) principles, that relies on the transcription of the software requirements into tests, before the code that passes the tests is written.\n",
    "\n",
    "It is an ** iterative process ** that aims at starting with a basic test case andthen ** upgrading alternatively ** the test cases and the code until the requirements are met, depending on the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing a test with `pytest`\n",
    "\n",
    "A unit test written under [`pytest`][pytest] is a Python function or class whose name **starts with \"`test`\"** and that makes an ** hypothesis considered true **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first test\n",
    "\n",
    "Let's right a basic file containing a function f, and the corresponding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file my_first_test.py\n",
    "\n",
    "def identity(a):\n",
    "    return a\n",
    "\n",
    "def test_a():\n",
    "    assert identity(1) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has been saved in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Info</b>: \n",
    "Shell commands can be run from the notebook preceded with an exclamation mark \"!\". This functionality will be used thoughout the notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching [pytest](http://pytest.org) is as easy as move to the right directory and using the command line\n",
    "\n",
    "    py.test\n",
    "    \n",
    "It will start a recursive search from the current directory for Python files, look for methods containing \"test\" and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test my_first_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick summary, use the quick option `-q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -q my_first_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on which test has been run, use the verbose option `-v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -v my_first_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional tests\n",
    "\n",
    "Let's now write a bunch of tests, introduce an error on `test_b` and re-run pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Info</b>: In a notebook cell, the <code>%%file path/to/filename.py</code> magic command will write the remainder of the cell into the file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file my_second_test.py\n",
    "\n",
    "def identity(a):\n",
    "    return a\n",
    "\n",
    "def test_a():\n",
    "    assert identity(1) == 1\n",
    "    \n",
    "def test_b():\n",
    "    assert identity(2) == 1\n",
    "\n",
    "def test_c():\n",
    "    assert identity(3) == 1 + 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -v my_second_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see pytest has *collected* and run 4 items, 1 from the first file, and 3 from the second. \n",
    "\n",
    "As expected, one test has failed.\n",
    "\n",
    "Therefore [pytest](http://pytest.org) shows the full traceback leading to the failure, and even gives the output value of the `f` method which can be useful for quick debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing errors and exceptions\n",
    "\n",
    "The philosophy of Python is to try something first and then decide what to do in case of an error. This is the reason behind Python Exceptions. They inform on the issue that was detected and help the user debug or catch it and find another way to deal with the issue.\n",
    "\n",
    "When testing a code, it is thus important to assess if these Exceptions are raised as they should be. However, since an exception raised but not caught in an environmment triggers an error, one cannot use the \"assert\" syntax but the context manager `pytest.raises` instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file my_third_test.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "def positive_identity(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Negative value detected\")\n",
    "    return n\n",
    "        \n",
    "def test_positive_identity():\n",
    "    assert positive_identity(1) == 1\n",
    "    \n",
    "def test_exception_positive_identity():\n",
    "    with pytest.raises(ValueError):\n",
    "        positive_identity(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -v my_third_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `numpy` and `pandas` testing helpers\n",
    "\n",
    "Both [numpy]() and [pandas]() libraries have created helper methods to ease the testing and comparison of their core objects, that is numpy `array` and pandas `DataFrame`.\n",
    "\n",
    "These methods can be found in a submodule called `testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.testing\n",
    "import pandas.testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of NumPy assert methods\n",
    "[func for func in dir(numpy.testing) if func.startswith('assert')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Pandas assert methods\n",
    "[func for func in dir(pandas.testing) if func.startswith('assert')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy assert methods will be used to write the tests in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean current directory before next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf my*test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing tests for the `spectra_analysis` module\n",
    "\n",
    "This part explains how to organize your tests for testing a module. It uses the `spectra_analysis` module created and upgrade all along this course. \n",
    "\n",
    "We will write tests for the two submodules:\n",
    "- `spectra_analysis/preprocessing.py`\n",
    "- `spectra_analysis/regression.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory setup\n",
    "\n",
    "We'll start by copying a clean version of the `spectra_analysis` module in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning current directory\n",
    "!rm -rf spectra_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy clean version from the previous course\n",
    "!cp -vr solutions/04_modules/spectra_analysis ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll create the structure for the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test directory\n",
    "!mkdir spectra_analysis/tests\n",
    "\n",
    "# __init__.py file so the directory is recognized as a submodule\n",
    "!touch spectra_analysis/tests/__init__.py\n",
    "\n",
    "# create a local copy of test data\n",
    "!mkdir spectra_analysis/tests/data\n",
    "!cp solutions/data/data.csv spectra_analysis/tests/data/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `preprocessing.py`\n",
    "\n",
    "We start first with the tests for the `read_spectra` method of the `preprocessing` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spectra_analysis.preprocessing import read_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a closer look at the content of the method, use the \"??\" syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_spectra??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a file called `test_preprocessing.py` with two unit tests: one for testing the [normal conditions](#Normal-conditions), the other one for handling the [exceptions](#Abnormal-conditions-and-edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file spectra_analysis/tests/test_preprocessing.py\n",
    "\n",
    "from spectra_analysis.preprocessing import read_spectra\n",
    "\n",
    "\n",
    "def test_read_spectra():\n",
    "    # Write here the tests for the normal conditions\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_read_spectra_exceptions():\n",
    "    # Write here the tests to handle errors\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>:<br>\n",
    "\n",
    "Fill the blanks in the two test units.<br><br>\n",
    "\n",
    "Advice:\n",
    "\n",
    "<ul>\n",
    "  <li>Use the example <code>data.csv</code> file</li>\n",
    "  <li>List all the outputs of the method and check them separately.</li>\n",
    "  <li>List the possible errors raised by the code and make the code fail to that these errors are handled.</li>\n",
    "  <li>Find the most appropriate assert method from numpy or pandas.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the tests pass by running pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -v spectra_analysis/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now focus on the second file `regression.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectra_analysis.regression import fit_params, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will create a file called `test_regression.py` with two unit tests corresponding to the methods `fit_params` and `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file spectra_analysis/tests/test_regression.py\n",
    "\n",
    "from spectra_analysis.regression import fit_params\n",
    "from spectra_analysis.regression import transform\n",
    "\n",
    "\n",
    "def test_fit_params():\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_transform():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>:<br>\n",
    "Same as above, implement tests for the two methods.<br><br>\n",
    "\n",
    "Advice:\n",
    "\n",
    "<ul>\n",
    "  <li>Start by defining a common array for both tests\n",
    "  <pre><code class=\"python language-python\">\n",
    "  X = np.array([[0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [1, 1, 1],\n",
    "                [1, 1, 1],\n",
    "                [1, 1, 1],\n",
    "                [2, 2, 2]])\n",
    "  </code></pre>\n",
    "                  </li>\n",
    "  <li>Compute the expected output of the methods applied on this array</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, verify that the tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -v spectra_analysis/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test coverage\n",
    "\n",
    "A test coverage is a report on the percentage of lines of a module that have been run during a test. \n",
    "\n",
    "The higher the coverage, the greater the number of code lines that have been executed at least once during a test.\n",
    "\n",
    "### Installation\n",
    "\n",
    "To use coverage with pytest, one must first install pytest coverage plugin, e.g. the `pytest-cov` library:\n",
    "    \n",
    "    conda install pytest-cov\n",
    "    \n",
    "or\n",
    "\n",
    "    pip install pytest-cov\n",
    "    \n",
    "    \n",
    "### Usage\n",
    "\n",
    "The coverage can then be run alongside the testing, by setting the path to the module to run the coverage against with an extra argument `--cov=<path to the module>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test -v --cov=spectra_analysis spectra_analysis/tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coverage table lists for each file in the module tree \n",
    "\n",
    "- **`Stmts`:** the number of actual lines of code in the file,\n",
    "- **`Miss`:** the number of code lines missed by the tests,\n",
    "- **`Cover`:** the resulting coverage percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE</b>:<br>\n",
    "Maximise the coverage of the two test files above.<br><br>\n",
    "\n",
    "Advice:\n",
    " <ul>\n",
    "  <li>You should obtain a coverage above 80%.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
